{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hY2ys9dWnAGI"
   },
   "source": [
    "# 데이터셋 불러오기\n",
    "- 데이터셋은 torchvision 패키지에서 제공하는 STL10 dataset을 사용함\n",
    "- STL10 dataset은 10개의 label을 가짐\n",
    "\n",
    "# Study Log\n",
    "- [1. [논문 구현] PyTorch로 GoogLeNet(2014) 구현하고 학습하기](https://deep-learning-study.tistory.com/523)\n",
    "    - 학습이 안됨..\n",
    "- [2. Paperswithcode(bisakhmondal) - GoogLeNet](https://github.com/pytorch/vision/blob/6db1569c89094cf23f3bc41f79275c45e9fcb3f3/torchvision/models/googlenet.py#L62)\n",
    "- [3. Paperswithcode(datumbox) - GoogLeNet](https://github.com/pytorch/vision/blob/main/torchvision/models/googlenet.py)\n",
    "\n",
    "- **220411 mon**\n",
    "    - 방법 1로는 학습이 되지 않는 문제 -> 방법 2를 통해 해결 시도\n",
    "    - train, valid, test 3가지로 split하는 것 구현\n",
    "    - earlystopping 유무 시의 출력의 차이 비교\n",
    "    - model을 정의하는 부분에서 softmax값을 반환하던 LeNet-5와 return값에 차이가 있음\n",
    "        - 이 때문에 같은 model(X)라도 return값이 달라져서 loss_func이 제대로 작동 못하는 중\n",
    "    - `(2017) DenseNet.ipynb`에서 train_val 코드 정상적으로 작동하니.. 구글넷에도 적용해보자\n",
    "\n",
    "- **220413 wed**\n",
    "    - 학습이 안됐던 문제가 어쩌면 모델의 return값의 문제일수도..?\n",
    "    - 학습 방법 4가지에서 `cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple` 문제를 지적중\n",
    "    - googlenet 방법 4 구현하고 jupyter에서 돌리니 거의 실행이 안되고(느림의 극대화), colab은 메모리 에러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T04:26:14.816614Z",
     "start_time": "2022-04-13T04:26:12.903366Z"
    },
    "id": "gD7SPL0GisSt"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# dataset and transformation\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# display images\n",
    "from torchvision import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# utils\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# 결과 확인을 용이하게 하기 위한 코드\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-13T03:09:09.679Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iyxt1mp9nZYF",
    "outputId": "957d6bea-3e8e-463a-c828-e0bdf393b982"
   },
   "outputs": [],
   "source": [
    "# specify the data path\n",
    "path2data = './data'\n",
    "\n",
    "# if not exists the path, make the directory\n",
    "if not os.path.exists(path2data):\n",
    "    os.mkdir(path2data)\n",
    "\n",
    "# load dataset\n",
    "train_ds = datasets.STL10(path2data, split='test', download=True, transform=transforms.ToTensor())\n",
    "test_ds = datasets.STL10(path2data, split='train', download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# train, valid, test split\n",
    "from torch.utils.data.dataset import random_split\n",
    "test_ds, val_ds = random_split(test_ds, [3000, 2000])\n",
    "\n",
    "print(len(train_ds))\n",
    "print(len(val_ds))\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-13T03:09:09.680Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4cMeMAh4qXuY",
    "outputId": "ec2a680f-8988-4194-8486-2847ee31a31b"
   },
   "outputs": [],
   "source": [
    "# To normalize the dataset, calculate the mean and std\n",
    "# train\n",
    "train_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in train_ds]\n",
    "train_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in train_ds]\n",
    "\n",
    "train_meanR = np.mean([m[0] for m in train_meanRGB])\n",
    "train_meanG = np.mean([m[1] for m in train_meanRGB])\n",
    "train_meanB = np.mean([m[2] for m in train_meanRGB])\n",
    "train_stdR = np.mean([s[0] for s in train_stdRGB])\n",
    "train_stdG = np.mean([s[1] for s in train_stdRGB])\n",
    "train_stdB = np.mean([s[2] for s in train_stdRGB])\n",
    "\n",
    "# vaild\n",
    "val_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in val_ds]\n",
    "val_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in val_ds]\n",
    "\n",
    "val_meanR = np.mean([m[0] for m in val_meanRGB])\n",
    "val_meanG = np.mean([m[1] for m in val_meanRGB])\n",
    "val_meanB = np.mean([m[2] for m in val_meanRGB])\n",
    "\n",
    "val_stdR = np.mean([s[0] for s in val_stdRGB])\n",
    "val_stdG = np.mean([s[1] for s in val_stdRGB])\n",
    "val_stdB = np.mean([s[2] for s in val_stdRGB])\n",
    "\n",
    "# test\n",
    "test_meanRGB = [np.mean(x.numpy(), axis=(1,2)) for x, _ in test_ds]\n",
    "test_stdRGB = [np.std(x.numpy(), axis=(1,2)) for x, _ in test_ds]\n",
    "\n",
    "test_meanR = np.mean([m[0] for m in test_meanRGB])\n",
    "test_meanG = np.mean([m[1] for m in test_meanRGB])\n",
    "test_meanB = np.mean([m[2] for m in test_meanRGB])\n",
    "\n",
    "test_stdR = np.mean([s[0] for s in test_stdRGB])\n",
    "test_stdG = np.mean([s[1] for s in test_stdRGB])\n",
    "test_stdB = np.mean([s[2] for s in test_stdRGB])\n",
    "\n",
    "print(train_meanR, train_meanG, train_meanB)\n",
    "print(val_meanR, val_meanG, val_meanB)\n",
    "print(test_meanR, test_meanG, test_meanB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-13T03:09:09.682Z"
    },
    "id": "khzDpyYKsCeR"
   },
   "outputs": [],
   "source": [
    "# define the image transformation\n",
    "train_transformation = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Resize(224),\n",
    "                        transforms.Normalize([train_meanR, train_meanG, train_meanB],\n",
    "                                             [train_stdR, train_stdG, train_stdB]),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "])\n",
    "\n",
    "val_transformation = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Resize(224),\n",
    "                        transforms.Normalize([val_meanR, val_meanG, val_meanB],\n",
    "                                             [val_stdR, val_stdG, val_stdB]),\n",
    "])\n",
    "\n",
    "test_transformation = transforms.Compose([\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Resize(224),\n",
    "                        transforms.Normalize([test_meanR, test_meanG, test_meanB],\n",
    "                                             [test_stdR, test_stdG, test_stdB]),\n",
    "])\n",
    "\n",
    "# apply transforamtion\n",
    "train_ds.transform = train_transformation\n",
    "val_ds.transform = val_transformation\n",
    "test_ds.tramsform = test_transformation\n",
    "\n",
    "# create DataLoader\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-13T03:09:09.684Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "Dp7QHiovzuzC",
    "outputId": "cb60c0e6-0090-4b28-ccff-8e448cbeceb9"
   },
   "outputs": [],
   "source": [
    "# display sample images\n",
    "def show(img, y=None, color=True):\n",
    "#     npimg = img.numpy()\n",
    "    npimg = img.detach().numpy()\n",
    "    npimg_tr = np.transpose(npimg, (1, 2, 0))\n",
    "    plt.imshow(npimg_tr)\n",
    "\n",
    "    if y is not None:\n",
    "        plt.title('labels: ' + str(y))\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "grid_size=4\n",
    "rnd_inds=np.random.randint(0,len(train_ds),grid_size)\n",
    "print(\"image indices:\",rnd_inds)\n",
    "\n",
    "x_grid=[train_ds[i][0] for i in rnd_inds]\n",
    "y_grid=[train_ds[i][1] for i in rnd_inds]\n",
    "\n",
    "x_grid=utils.make_grid(x_grid, nrow=4, padding=2)\n",
    "print(x_grid.shape)\n",
    "\n",
    "# call helper function\n",
    "plt.figure(figsize=(10,10))\n",
    "show(x_grid,y_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNPtvqI0JhDy"
   },
   "source": [
    "# 모델 구축하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ver1.\n",
    "- [1. [논문 구현] PyTorch로 GoogLeNet(2014) 구현하고 학습하기](https://deep-learning-study.tistory.com/523)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T02:21:22.919284Z",
     "start_time": "2022-04-13T02:21:22.855754Z"
    },
    "id": "8sxnPKarJqGu"
   },
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self,aux_logits=True, num_classes=10, init_weights=True):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        assert aux_logits == True or aux_logits == False\n",
    "        self.aux_logits = aux_logits\n",
    "\n",
    "        # conv_block takes in_channels, out_channels, kernel_size, stride, padding\n",
    "        # Inception block takes out1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool\n",
    "\n",
    "        self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.maxpool1 = nn.MaxPool2d(3, 2, 1)\n",
    "        self.conv2 = conv_block(64, 192, kernel_size=3, stride=1, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool2d(3, 2, 1)\n",
    "        self.inception3a = Inception_block(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = Inception_block(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(3, 2, 1)\n",
    "        self.inception4a = Inception_block(480, 192, 96, 208, 16, 48, 64)\n",
    "\n",
    "        # auxiliary classifier\n",
    "\n",
    "        self.inception4b = Inception_block(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = Inception_block(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = Inception_block(512, 112, 144, 288, 32, 64, 64)\n",
    "\n",
    "        # auxiliary classifier\n",
    "\n",
    "        self.inception4e = Inception_block(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(3, 2, 1)\n",
    "        self.inception5a = Inception_block(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = Inception_block(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(7, 1)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.fc1 = nn.Linear(1024, num_classes)\n",
    "\n",
    "        if self.aux_logits:\n",
    "            self.aux1 = InceptionAux(512, num_classes)\n",
    "            self.aux2 = InceptionAux(528, num_classes)\n",
    "        else:\n",
    "            self.aux1 = self.aux2 = None\n",
    "\n",
    "        # weight initialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.inception4a(x)\n",
    "\n",
    "        if self.aux_logits and self.training:\n",
    "            aux1 = self.aux1(x)\n",
    "\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "\n",
    "        if self.aux_logits and self.training:\n",
    "            aux2 = self.aux2(x)\n",
    "\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        x = self.avgpool(x)\n",
    "\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        if self.aux_logits and self.training:\n",
    "            return x, aux1, aux2\n",
    "        else:\n",
    "            return x \n",
    "\n",
    "    # define weight initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(conv_block, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv_layer(x)\n",
    "\n",
    "\n",
    "class Inception_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool):\n",
    "        super(Inception_block, self).__init__()\n",
    "\n",
    "        self.branch1 = conv_block(in_channels, out_1x1, kernel_size=1)\n",
    "\n",
    "        self.branch2 = nn.Sequential(\n",
    "            conv_block(in_channels, red_3x3, kernel_size=1),\n",
    "            conv_block(red_3x3, out_3x3, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.branch3 = nn.Sequential(\n",
    "            conv_block(in_channels, red_5x5, kernel_size=1),\n",
    "            conv_block(red_5x5, out_5x5, kernel_size=5, padding=2),\n",
    "        )\n",
    "\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            conv_block(in_channels, out_1x1pool, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 0차원은 batch이므로 1차원인 filter 수를 기준으로 각 branch의 출력값을 묶어줍니다. \n",
    "        x = torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], 1)\n",
    "        return x\n",
    "\n",
    "# auxiliary classifier의 loss는 0.3이 곱해지고, 최종 loss에 추가합니다. 정규화 효과가 있습니다. \n",
    "class InceptionAux(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(InceptionAux, self).__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=5, stride=3),\n",
    "            conv_block(in_channels, 128, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T02:25:32.559662Z",
     "start_time": "2022-04-13T02:25:32.443811Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FPR81QaO1jKt",
    "outputId": "9318b054-1fc4-47fb-dff5-b2b329342c70"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(device)\n",
    "\n",
    "model = GoogLeNet(aux_logits=True, num_classes=10, init_weights=True).to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T02:26:06.320797Z",
     "start_time": "2022-04-13T02:26:06.090078Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ZOYqwHFAXW4",
    "outputId": "c8261f09-0019-45b5-ffbd-e466ca7b1943",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0.0635,  0.1731,  0.0441,  0.3331, -0.1027,  0.2278,  0.1009,  0.3052,\n",
      "          0.0830, -0.2049],\n",
      "        [ 0.0993,  0.1403,  0.1545,  0.2341, -0.1739,  0.0469,  0.0796,  0.0914,\n",
      "          0.0869, -0.2469],\n",
      "        [ 0.2751,  0.0830,  0.3790,  0.2204,  0.0895,  0.0776,  0.0070,  0.0279,\n",
      "         -0.0158, -0.2440]], grad_fn=<AddmmBackward0>), tensor([[-0.0266, -0.1937, -0.0435,  0.0596,  0.0588, -0.0707,  0.0990, -0.0707,\n",
      "         -0.0160, -0.1358],\n",
      "        [-0.0317, -0.0457,  0.0504,  0.0843,  0.0017,  0.0045, -0.1274,  0.1979,\n",
      "         -0.0538, -0.0154],\n",
      "        [-0.0208, -0.0538,  0.0119,  0.0531,  0.1507,  0.0035, -0.1153,  0.1164,\n",
      "          0.0355,  0.0114]], grad_fn=<AddmmBackward0>), tensor([[ 0.0668,  0.0025, -0.1292,  0.1469,  0.1411, -0.1215, -0.0313,  0.0164,\n",
      "         -0.0495,  0.1039],\n",
      "        [ 0.0247,  0.1687, -0.0228,  0.0824,  0.0183,  0.0762, -0.0814,  0.0718,\n",
      "         -0.0387,  0.0657],\n",
      "        [ 0.0754,  0.0559, -0.1146, -0.0559, -0.0465, -0.0313, -0.1334, -0.0014,\n",
      "         -0.0460,  0.1209]], grad_fn=<AddmmBackward0>))\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 3, 224, 224).to(device)\n",
    "output = model(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-13T02:21:23.508495Z",
     "start_time": "2022-04-13T02:21:23.306885Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFirgHcSUyE9",
    "outputId": "70f7fa3c-52d9-4824-8157-bc1db1647fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,472\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "        conv_block-4         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-5           [-1, 64, 56, 56]               0\n",
      "            Conv2d-6          [-1, 192, 56, 56]         110,784\n",
      "       BatchNorm2d-7          [-1, 192, 56, 56]             384\n",
      "              ReLU-8          [-1, 192, 56, 56]               0\n",
      "        conv_block-9          [-1, 192, 56, 56]               0\n",
      "        MaxPool2d-10          [-1, 192, 28, 28]               0\n",
      "           Conv2d-11           [-1, 64, 28, 28]          12,352\n",
      "      BatchNorm2d-12           [-1, 64, 28, 28]             128\n",
      "             ReLU-13           [-1, 64, 28, 28]               0\n",
      "       conv_block-14           [-1, 64, 28, 28]               0\n",
      "           Conv2d-15           [-1, 96, 28, 28]          18,528\n",
      "      BatchNorm2d-16           [-1, 96, 28, 28]             192\n",
      "             ReLU-17           [-1, 96, 28, 28]               0\n",
      "       conv_block-18           [-1, 96, 28, 28]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]         110,720\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "       conv_block-22          [-1, 128, 28, 28]               0\n",
      "           Conv2d-23           [-1, 16, 28, 28]           3,088\n",
      "      BatchNorm2d-24           [-1, 16, 28, 28]              32\n",
      "             ReLU-25           [-1, 16, 28, 28]               0\n",
      "       conv_block-26           [-1, 16, 28, 28]               0\n",
      "           Conv2d-27           [-1, 32, 28, 28]          12,832\n",
      "      BatchNorm2d-28           [-1, 32, 28, 28]              64\n",
      "             ReLU-29           [-1, 32, 28, 28]               0\n",
      "       conv_block-30           [-1, 32, 28, 28]               0\n",
      "        MaxPool2d-31          [-1, 192, 28, 28]               0\n",
      "           Conv2d-32           [-1, 32, 28, 28]           6,176\n",
      "      BatchNorm2d-33           [-1, 32, 28, 28]              64\n",
      "             ReLU-34           [-1, 32, 28, 28]               0\n",
      "       conv_block-35           [-1, 32, 28, 28]               0\n",
      "  Inception_block-36          [-1, 256, 28, 28]               0\n",
      "           Conv2d-37          [-1, 128, 28, 28]          32,896\n",
      "      BatchNorm2d-38          [-1, 128, 28, 28]             256\n",
      "             ReLU-39          [-1, 128, 28, 28]               0\n",
      "       conv_block-40          [-1, 128, 28, 28]               0\n",
      "           Conv2d-41          [-1, 128, 28, 28]          32,896\n",
      "      BatchNorm2d-42          [-1, 128, 28, 28]             256\n",
      "             ReLU-43          [-1, 128, 28, 28]               0\n",
      "       conv_block-44          [-1, 128, 28, 28]               0\n",
      "           Conv2d-45          [-1, 192, 28, 28]         221,376\n",
      "      BatchNorm2d-46          [-1, 192, 28, 28]             384\n",
      "             ReLU-47          [-1, 192, 28, 28]               0\n",
      "       conv_block-48          [-1, 192, 28, 28]               0\n",
      "           Conv2d-49           [-1, 32, 28, 28]           8,224\n",
      "      BatchNorm2d-50           [-1, 32, 28, 28]              64\n",
      "             ReLU-51           [-1, 32, 28, 28]               0\n",
      "       conv_block-52           [-1, 32, 28, 28]               0\n",
      "           Conv2d-53           [-1, 96, 28, 28]          76,896\n",
      "      BatchNorm2d-54           [-1, 96, 28, 28]             192\n",
      "             ReLU-55           [-1, 96, 28, 28]               0\n",
      "       conv_block-56           [-1, 96, 28, 28]               0\n",
      "        MaxPool2d-57          [-1, 256, 28, 28]               0\n",
      "           Conv2d-58           [-1, 64, 28, 28]          16,448\n",
      "      BatchNorm2d-59           [-1, 64, 28, 28]             128\n",
      "             ReLU-60           [-1, 64, 28, 28]               0\n",
      "       conv_block-61           [-1, 64, 28, 28]               0\n",
      "  Inception_block-62          [-1, 480, 28, 28]               0\n",
      "        MaxPool2d-63          [-1, 480, 14, 14]               0\n",
      "           Conv2d-64          [-1, 192, 14, 14]          92,352\n",
      "      BatchNorm2d-65          [-1, 192, 14, 14]             384\n",
      "             ReLU-66          [-1, 192, 14, 14]               0\n",
      "       conv_block-67          [-1, 192, 14, 14]               0\n",
      "           Conv2d-68           [-1, 96, 14, 14]          46,176\n",
      "      BatchNorm2d-69           [-1, 96, 14, 14]             192\n",
      "             ReLU-70           [-1, 96, 14, 14]               0\n",
      "       conv_block-71           [-1, 96, 14, 14]               0\n",
      "           Conv2d-72          [-1, 208, 14, 14]         179,920\n",
      "      BatchNorm2d-73          [-1, 208, 14, 14]             416\n",
      "             ReLU-74          [-1, 208, 14, 14]               0\n",
      "       conv_block-75          [-1, 208, 14, 14]               0\n",
      "           Conv2d-76           [-1, 16, 14, 14]           7,696\n",
      "      BatchNorm2d-77           [-1, 16, 14, 14]              32\n",
      "             ReLU-78           [-1, 16, 14, 14]               0\n",
      "       conv_block-79           [-1, 16, 14, 14]               0\n",
      "           Conv2d-80           [-1, 48, 14, 14]          19,248\n",
      "      BatchNorm2d-81           [-1, 48, 14, 14]              96\n",
      "             ReLU-82           [-1, 48, 14, 14]               0\n",
      "       conv_block-83           [-1, 48, 14, 14]               0\n",
      "        MaxPool2d-84          [-1, 480, 14, 14]               0\n",
      "           Conv2d-85           [-1, 64, 14, 14]          30,784\n",
      "      BatchNorm2d-86           [-1, 64, 14, 14]             128\n",
      "             ReLU-87           [-1, 64, 14, 14]               0\n",
      "       conv_block-88           [-1, 64, 14, 14]               0\n",
      "  Inception_block-89          [-1, 512, 14, 14]               0\n",
      "        AvgPool2d-90            [-1, 512, 4, 4]               0\n",
      "           Conv2d-91            [-1, 128, 4, 4]          65,664\n",
      "      BatchNorm2d-92            [-1, 128, 4, 4]             256\n",
      "             ReLU-93            [-1, 128, 4, 4]               0\n",
      "       conv_block-94            [-1, 128, 4, 4]               0\n",
      "           Linear-95                 [-1, 1024]       2,098,176\n",
      "             ReLU-96                 [-1, 1024]               0\n",
      "          Dropout-97                 [-1, 1024]               0\n",
      "           Linear-98                   [-1, 10]          10,250\n",
      "     InceptionAux-99                   [-1, 10]               0\n",
      "          Conv2d-100          [-1, 160, 14, 14]          82,080\n",
      "     BatchNorm2d-101          [-1, 160, 14, 14]             320\n",
      "            ReLU-102          [-1, 160, 14, 14]               0\n",
      "      conv_block-103          [-1, 160, 14, 14]               0\n",
      "          Conv2d-104          [-1, 112, 14, 14]          57,456\n",
      "     BatchNorm2d-105          [-1, 112, 14, 14]             224\n",
      "            ReLU-106          [-1, 112, 14, 14]               0\n",
      "      conv_block-107          [-1, 112, 14, 14]               0\n",
      "          Conv2d-108          [-1, 224, 14, 14]         226,016\n",
      "     BatchNorm2d-109          [-1, 224, 14, 14]             448\n",
      "            ReLU-110          [-1, 224, 14, 14]               0\n",
      "      conv_block-111          [-1, 224, 14, 14]               0\n",
      "          Conv2d-112           [-1, 24, 14, 14]          12,312\n",
      "     BatchNorm2d-113           [-1, 24, 14, 14]              48\n",
      "            ReLU-114           [-1, 24, 14, 14]               0\n",
      "      conv_block-115           [-1, 24, 14, 14]               0\n",
      "          Conv2d-116           [-1, 64, 14, 14]          38,464\n",
      "     BatchNorm2d-117           [-1, 64, 14, 14]             128\n",
      "            ReLU-118           [-1, 64, 14, 14]               0\n",
      "      conv_block-119           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-120          [-1, 512, 14, 14]               0\n",
      "          Conv2d-121           [-1, 64, 14, 14]          32,832\n",
      "     BatchNorm2d-122           [-1, 64, 14, 14]             128\n",
      "            ReLU-123           [-1, 64, 14, 14]               0\n",
      "      conv_block-124           [-1, 64, 14, 14]               0\n",
      " Inception_block-125          [-1, 512, 14, 14]               0\n",
      "          Conv2d-126          [-1, 128, 14, 14]          65,664\n",
      "     BatchNorm2d-127          [-1, 128, 14, 14]             256\n",
      "            ReLU-128          [-1, 128, 14, 14]               0\n",
      "      conv_block-129          [-1, 128, 14, 14]               0\n",
      "          Conv2d-130          [-1, 128, 14, 14]          65,664\n",
      "     BatchNorm2d-131          [-1, 128, 14, 14]             256\n",
      "            ReLU-132          [-1, 128, 14, 14]               0\n",
      "      conv_block-133          [-1, 128, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         295,168\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "      conv_block-137          [-1, 256, 14, 14]               0\n",
      "          Conv2d-138           [-1, 24, 14, 14]          12,312\n",
      "     BatchNorm2d-139           [-1, 24, 14, 14]              48\n",
      "            ReLU-140           [-1, 24, 14, 14]               0\n",
      "      conv_block-141           [-1, 24, 14, 14]               0\n",
      "          Conv2d-142           [-1, 64, 14, 14]          38,464\n",
      "     BatchNorm2d-143           [-1, 64, 14, 14]             128\n",
      "            ReLU-144           [-1, 64, 14, 14]               0\n",
      "      conv_block-145           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-146          [-1, 512, 14, 14]               0\n",
      "          Conv2d-147           [-1, 64, 14, 14]          32,832\n",
      "     BatchNorm2d-148           [-1, 64, 14, 14]             128\n",
      "            ReLU-149           [-1, 64, 14, 14]               0\n",
      "      conv_block-150           [-1, 64, 14, 14]               0\n",
      " Inception_block-151          [-1, 512, 14, 14]               0\n",
      "          Conv2d-152          [-1, 112, 14, 14]          57,456\n",
      "     BatchNorm2d-153          [-1, 112, 14, 14]             224\n",
      "            ReLU-154          [-1, 112, 14, 14]               0\n",
      "      conv_block-155          [-1, 112, 14, 14]               0\n",
      "          Conv2d-156          [-1, 144, 14, 14]          73,872\n",
      "     BatchNorm2d-157          [-1, 144, 14, 14]             288\n",
      "            ReLU-158          [-1, 144, 14, 14]               0\n",
      "      conv_block-159          [-1, 144, 14, 14]               0\n",
      "          Conv2d-160          [-1, 288, 14, 14]         373,536\n",
      "     BatchNorm2d-161          [-1, 288, 14, 14]             576\n",
      "            ReLU-162          [-1, 288, 14, 14]               0\n",
      "      conv_block-163          [-1, 288, 14, 14]               0\n",
      "          Conv2d-164           [-1, 32, 14, 14]          16,416\n",
      "     BatchNorm2d-165           [-1, 32, 14, 14]              64\n",
      "            ReLU-166           [-1, 32, 14, 14]               0\n",
      "      conv_block-167           [-1, 32, 14, 14]               0\n",
      "          Conv2d-168           [-1, 64, 14, 14]          51,264\n",
      "     BatchNorm2d-169           [-1, 64, 14, 14]             128\n",
      "            ReLU-170           [-1, 64, 14, 14]               0\n",
      "      conv_block-171           [-1, 64, 14, 14]               0\n",
      "       MaxPool2d-172          [-1, 512, 14, 14]               0\n",
      "          Conv2d-173           [-1, 64, 14, 14]          32,832\n",
      "     BatchNorm2d-174           [-1, 64, 14, 14]             128\n",
      "            ReLU-175           [-1, 64, 14, 14]               0\n",
      "      conv_block-176           [-1, 64, 14, 14]               0\n",
      " Inception_block-177          [-1, 528, 14, 14]               0\n",
      "       AvgPool2d-178            [-1, 528, 4, 4]               0\n",
      "          Conv2d-179            [-1, 128, 4, 4]          67,712\n",
      "     BatchNorm2d-180            [-1, 128, 4, 4]             256\n",
      "            ReLU-181            [-1, 128, 4, 4]               0\n",
      "      conv_block-182            [-1, 128, 4, 4]               0\n",
      "          Linear-183                 [-1, 1024]       2,098,176\n",
      "            ReLU-184                 [-1, 1024]               0\n",
      "         Dropout-185                 [-1, 1024]               0\n",
      "          Linear-186                   [-1, 10]          10,250\n",
      "    InceptionAux-187                   [-1, 10]               0\n",
      "          Conv2d-188          [-1, 256, 14, 14]         135,424\n",
      "     BatchNorm2d-189          [-1, 256, 14, 14]             512\n",
      "            ReLU-190          [-1, 256, 14, 14]               0\n",
      "      conv_block-191          [-1, 256, 14, 14]               0\n",
      "          Conv2d-192          [-1, 160, 14, 14]          84,640\n",
      "     BatchNorm2d-193          [-1, 160, 14, 14]             320\n",
      "            ReLU-194          [-1, 160, 14, 14]               0\n",
      "      conv_block-195          [-1, 160, 14, 14]               0\n",
      "          Conv2d-196          [-1, 320, 14, 14]         461,120\n",
      "     BatchNorm2d-197          [-1, 320, 14, 14]             640\n",
      "            ReLU-198          [-1, 320, 14, 14]               0\n",
      "      conv_block-199          [-1, 320, 14, 14]               0\n",
      "          Conv2d-200           [-1, 32, 14, 14]          16,928\n",
      "     BatchNorm2d-201           [-1, 32, 14, 14]              64\n",
      "            ReLU-202           [-1, 32, 14, 14]               0\n",
      "      conv_block-203           [-1, 32, 14, 14]               0\n",
      "          Conv2d-204          [-1, 128, 14, 14]         102,528\n",
      "     BatchNorm2d-205          [-1, 128, 14, 14]             256\n",
      "            ReLU-206          [-1, 128, 14, 14]               0\n",
      "      conv_block-207          [-1, 128, 14, 14]               0\n",
      "       MaxPool2d-208          [-1, 528, 14, 14]               0\n",
      "          Conv2d-209          [-1, 128, 14, 14]          67,712\n",
      "     BatchNorm2d-210          [-1, 128, 14, 14]             256\n",
      "            ReLU-211          [-1, 128, 14, 14]               0\n",
      "      conv_block-212          [-1, 128, 14, 14]               0\n",
      " Inception_block-213          [-1, 832, 14, 14]               0\n",
      "       MaxPool2d-214            [-1, 832, 7, 7]               0\n",
      "          Conv2d-215            [-1, 256, 7, 7]         213,248\n",
      "     BatchNorm2d-216            [-1, 256, 7, 7]             512\n",
      "            ReLU-217            [-1, 256, 7, 7]               0\n",
      "      conv_block-218            [-1, 256, 7, 7]               0\n",
      "          Conv2d-219            [-1, 160, 7, 7]         133,280\n",
      "     BatchNorm2d-220            [-1, 160, 7, 7]             320\n",
      "            ReLU-221            [-1, 160, 7, 7]               0\n",
      "      conv_block-222            [-1, 160, 7, 7]               0\n",
      "          Conv2d-223            [-1, 320, 7, 7]         461,120\n",
      "     BatchNorm2d-224            [-1, 320, 7, 7]             640\n",
      "            ReLU-225            [-1, 320, 7, 7]               0\n",
      "      conv_block-226            [-1, 320, 7, 7]               0\n",
      "          Conv2d-227             [-1, 32, 7, 7]          26,656\n",
      "     BatchNorm2d-228             [-1, 32, 7, 7]              64\n",
      "            ReLU-229             [-1, 32, 7, 7]               0\n",
      "      conv_block-230             [-1, 32, 7, 7]               0\n",
      "          Conv2d-231            [-1, 128, 7, 7]         102,528\n",
      "     BatchNorm2d-232            [-1, 128, 7, 7]             256\n",
      "            ReLU-233            [-1, 128, 7, 7]               0\n",
      "      conv_block-234            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-235            [-1, 832, 7, 7]               0\n",
      "          Conv2d-236            [-1, 128, 7, 7]         106,624\n",
      "     BatchNorm2d-237            [-1, 128, 7, 7]             256\n",
      "            ReLU-238            [-1, 128, 7, 7]               0\n",
      "      conv_block-239            [-1, 128, 7, 7]               0\n",
      " Inception_block-240            [-1, 832, 7, 7]               0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Conv2d-241            [-1, 384, 7, 7]         319,872\n",
      "     BatchNorm2d-242            [-1, 384, 7, 7]             768\n",
      "            ReLU-243            [-1, 384, 7, 7]               0\n",
      "      conv_block-244            [-1, 384, 7, 7]               0\n",
      "          Conv2d-245            [-1, 192, 7, 7]         159,936\n",
      "     BatchNorm2d-246            [-1, 192, 7, 7]             384\n",
      "            ReLU-247            [-1, 192, 7, 7]               0\n",
      "      conv_block-248            [-1, 192, 7, 7]               0\n",
      "          Conv2d-249            [-1, 384, 7, 7]         663,936\n",
      "     BatchNorm2d-250            [-1, 384, 7, 7]             768\n",
      "            ReLU-251            [-1, 384, 7, 7]               0\n",
      "      conv_block-252            [-1, 384, 7, 7]               0\n",
      "          Conv2d-253             [-1, 48, 7, 7]          39,984\n",
      "     BatchNorm2d-254             [-1, 48, 7, 7]              96\n",
      "            ReLU-255             [-1, 48, 7, 7]               0\n",
      "      conv_block-256             [-1, 48, 7, 7]               0\n",
      "          Conv2d-257            [-1, 128, 7, 7]         153,728\n",
      "     BatchNorm2d-258            [-1, 128, 7, 7]             256\n",
      "            ReLU-259            [-1, 128, 7, 7]               0\n",
      "      conv_block-260            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-261            [-1, 832, 7, 7]               0\n",
      "          Conv2d-262            [-1, 128, 7, 7]         106,624\n",
      "     BatchNorm2d-263            [-1, 128, 7, 7]             256\n",
      "            ReLU-264            [-1, 128, 7, 7]               0\n",
      "      conv_block-265            [-1, 128, 7, 7]               0\n",
      " Inception_block-266           [-1, 1024, 7, 7]               0\n",
      "       AvgPool2d-267           [-1, 1024, 1, 1]               0\n",
      "         Dropout-268                 [-1, 1024]               0\n",
      "          Linear-269                   [-1, 10]          10,250\n",
      "================================================================\n",
      "Total params: 10,344,814\n",
      "Trainable params: 10,344,814\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 112.89\n",
      "Params size (MB): 39.46\n",
      "Estimated Total Size (MB): 152.92\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3,224,224), device=device.type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ver2. \n",
    "- [[github] weiaicunzai/pytorch-cifar100/models](https://github.com/weiaicunzai/pytorch-cifar100/blob/master/models/googlenet.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-13T03:09:16.427Z"
    }
   },
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, input_channels, n1x1, n3x3_reduce, n3x3, n5x5_reduce, n5x5, pool_proj):\n",
    "        super().__init__()\n",
    "\n",
    "        #1x1conv branch\n",
    "        self.b1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, n1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(n1x1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        #1x1conv -> 3x3conv branch\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, n3x3_reduce, kernel_size=1),\n",
    "            nn.BatchNorm2d(n3x3_reduce),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(n3x3_reduce, n3x3, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n3x3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        #1x1conv -> 5x5conv branch\n",
    "        #we use 2 3x3 conv filters stacked instead\n",
    "        #of 1 5x5 filters to obtain the same receptive\n",
    "        #field with fewer parameters\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, n5x5_reduce, kernel_size=1),\n",
    "            nn.BatchNorm2d(n5x5_reduce),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(n5x5_reduce, n5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n5x5, n5x5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        #3x3pooling -> 1x1conv\n",
    "        #same conv\n",
    "        self.b4 = nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(input_channels, pool_proj, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_proj),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.b1(x), self.b2(x), self.b3(x), self.b4(x)], dim=1)\n",
    "\n",
    "\n",
    "class GoogleNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class=100):\n",
    "        super().__init__()\n",
    "        self.prelayer = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        #although we only use 1 conv layer as prelayer,\n",
    "        #we still use name a3, b3.......\n",
    "        self.a3 = Inception(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "        ##\"\"\"In general, an Inception network is a network consisting of\n",
    "        ##modules of the above type stacked upon each other, with occasional\n",
    "        ##max-pooling layers with stride 2 to halve the resolution of the\n",
    "        ##grid\"\"\"\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        self.a4 = Inception(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "\n",
    "        #input feature size: 8*8*1024\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.dropout = nn.Dropout2d(p=0.4)\n",
    "        self.linear = nn.Linear(1024, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prelayer(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.a3(x)\n",
    "        x = self.b3(x)\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.a4(x)\n",
    "        x = self.b4(x)\n",
    "        x = self.c4(x)\n",
    "        x = self.d4(x)\n",
    "        x = self.e4(x)\n",
    "\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.a5(x)\n",
    "        x = self.b5(x)\n",
    "\n",
    "        #\"\"\"It was found that a move from fully connected layers to\n",
    "        #average pooling improved the top-1 accuracy by about 0.6%,\n",
    "        #however the use of dropout remained essential even after\n",
    "        #removing the fully connected layers.\"\"\"\n",
    "        x = self.avgpool(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GoogleNet()\n",
    "model2 = GoogleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxaPU8vX3nto"
   },
   "source": [
    "# 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법 1\n",
    "- `./original/(2014) GoogLeNet.ipynb`에 있는 방법으로 하면 error 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T02:31:15.194823Z",
     "start_time": "2022-04-11T02:31:15.180253Z"
    },
    "id": "h3dOkwZR3ZcR"
   },
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "lr_scheduler = StepLR(opt, step_size=30, gamma=0.1)\n",
    "\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "\n",
    "def metric_batch(output, target):\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "\n",
    "def loss_batch(loss_func, outputs, target, opt=None):\n",
    "    if np.shape(outputs)[0] == 3:\n",
    "        output, aux1, aux2 = outputs\n",
    "\n",
    "        output_loss = loss_func(output, target)\n",
    "        aux1_loss = loss_func(aux1, target)\n",
    "        aux2_loss = loss_func(aux2, target)\n",
    "\n",
    "        loss = output_loss + 0.3*(aux1_loss + aux2_loss)\n",
    "        metric_b = metric_batch(output,target)\n",
    "\n",
    "    else:\n",
    "        loss = loss_func(outputs, target)\n",
    "        metric_b = metric_batch(outputs, target)\n",
    "\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    return loss.item(), metric_b\n",
    "\n",
    "\n",
    "\n",
    "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "\n",
    "    for xb, yb in dataset_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        output= model(xb)\n",
    "\n",
    "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
    "\n",
    "        running_loss += loss_b\n",
    "\n",
    "        if metric_b is not None:\n",
    "            running_metric += metric_b\n",
    "        \n",
    "        if sanity_check is True:\n",
    "            break\n",
    "\n",
    "    loss = running_loss / len_data\n",
    "    metric = running_metric / len_data\n",
    "\n",
    "    return loss, metric\n",
    "\n",
    "\n",
    "\n",
    "def train_val(model, params):\n",
    "    num_epochs=params[\"num_epochs\"]\n",
    "    loss_func=params[\"loss_func\"]\n",
    "    opt=params[\"optimizer\"]\n",
    "    train_dl=params[\"train_dl\"]\n",
    "    val_dl=params[\"val_dl\"]\n",
    "    sanity_check=params[\"sanity_check\"]\n",
    "    lr_scheduler=params[\"lr_scheduler\"]\n",
    "    path2weights=params[\"path2weights\"]\n",
    "\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    metric_history = {'train': [], 'val': []}\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr={}'.format(epoch, num_epochs - 1, current_lr))\n",
    "        \n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        metric_history['train'].append(train_metric)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print('Copied best model weights!')\n",
    "\n",
    "        loss_history['val'].append(val_loss)\n",
    "        metric_history['val'].append(val_metric)\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' \n",
    "              %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
    "        print('-'*10)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T02:31:24.486497Z",
     "start_time": "2022-04-11T02:31:24.482378Z"
    },
    "id": "Scb8WiNF7H-h"
   },
   "outputs": [],
   "source": [
    "# definc the training parameters\n",
    "params_train = {\n",
    "    'num_epochs':100,\n",
    "    'optimizer':opt,\n",
    "    'loss_func':loss_func,\n",
    "    'train_dl':train_dl,\n",
    "    'val_dl':val_dl,\n",
    "    'sanity_check':False,\n",
    "    'lr_scheduler':lr_scheduler,\n",
    "    'path2weights':'./models/weights.pt',\n",
    "}\n",
    "\n",
    "# create the directory that stores weights.pt\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSerror:\n",
    "        print('Error')\n",
    "createFolder('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T02:31:30.387751Z",
     "start_time": "2022-04-11T02:31:27.297810Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6YrgYRzE0nH",
    "outputId": "5c528213-b1d0-4ee8-e4c9-ce62d9f9dbd4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/99, current lr=0.001\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2005\u001b[0m, in \u001b[0;36mshape\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2005\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, loss_hist, metric_hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mtrain_val\u001b[0;34m(model, params)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, current lr=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, num_epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, current_lr))\n\u001b[1;32m     93\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 94\u001b[0m train_loss, train_metric \u001b[38;5;241m=\u001b[39m \u001b[43mloss_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanity_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m loss_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     96\u001b[0m metric_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_metric)\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mloss_epoch\u001b[0;34m(model, loss_func, dataset_dl, sanity_check, opt)\u001b[0m\n\u001b[1;32m     51\u001b[0m yb \u001b[38;5;241m=\u001b[39m yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m output\u001b[38;5;241m=\u001b[39m model(xb)\n\u001b[0;32m---> 54\u001b[0m loss_b, metric_b \u001b[38;5;241m=\u001b[39m \u001b[43mloss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_b\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_b \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mloss_batch\u001b[0;34m(loss_func, outputs, target, opt)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_batch\u001b[39m(loss_func, outputs, target, opt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     22\u001b[0m         output, aux1, aux2 \u001b[38;5;241m=\u001b[39m outputs\n\u001b[1;32m     24\u001b[0m         output_loss \u001b[38;5;241m=\u001b[39m loss_func(output, target)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2007\u001b[0m, in \u001b[0;36mshape\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   2005\u001b[0m     result \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 2007\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m   2008\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:732\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "model, loss_hist, metric_hist = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5ckytcrKtT0"
   },
   "outputs": [],
   "source": [
    "# Train-Validation Progress\n",
    "num_epochs=params_train[\"num_epochs\"]\n",
    "\n",
    "# plot loss progress\n",
    "plt.title(\"Train-Val Loss\")\n",
    "plt.plot(range(1,num_epochs+1),loss_hist[\"train\"],label=\"train\")\n",
    "plt.plot(range(1,num_epochs+1),loss_hist[\"val\"],label=\"val\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot accuracy progress\n",
    "plt.title(\"Train-Val Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),metric_hist[\"train\"],label=\"train\")\n",
    "plt.plot(range(1,num_epochs+1),metric_hist[\"val\"],label=\"val\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법 2\n",
    "- [CNN, data augmentation X (pytorch, test acc: 74.2)](https://dacon.io/competitions/official/235874/codeshare/4610?page=1&dtype=recent)에서 제시한 코드를 참고해서 구현해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T07:15:00.408670Z",
     "start_time": "2022-04-11T07:15:00.401366Z"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    ''' 주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지'''\n",
    "    \n",
    "    def __init__(self, patience = 7, verbose = False, delta = 0, path='checkpoint.pt'):\n",
    "        '''\n",
    "        Args:\n",
    "        patience(int) : validation loss가 개선된 후 기다리는 시간 (Default : 7)\n",
    "        verbose(bool) : True일 경우 각 validation loss의 개선 사항 메시지 출력 (Default : False)\n",
    "        delta(float) : 개선되었다고 인정되는 monitored quantity의 최소 변화 (Default : 0)\n",
    "        path (str) : checkpoint 저장 경로 (Default : 'checkpoint.pt')\n",
    "        '''\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "    \n",
    "    def _call_(self, val_loss, model):\n",
    "        score -= val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.check_point(val_loss, model)\n",
    "            \n",
    "        elif score < self.best_score + self.delta :\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter : {self.counter} out of {self.patience}')\n",
    "            \n",
    "            # 지정한 patience 횟수를 거쳤음에도 val_loss가 개선되지 않은 경우 earlystop 종료\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model)\n",
    "                self.counter = 0\n",
    "    \n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''validation loss가 감소하면 모델을 저장함'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss : .6f}). Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T07:33:11.862372Z",
     "start_time": "2022-04-11T07:33:11.857123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(val_dl)\n",
    "type(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T07:33:39.721188Z",
     "start_time": "2022-04-11T07:33:37.384921Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 70>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[1;32m     68\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m---> 70\u001b[0m model, train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, patience, loss_fn, optimizer, n_epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m     16\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[0;32m---> 17\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2995\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "def fit(model, patience, loss_fn, optimizer, n_epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    avg_train_loss = []\n",
    "    avg_valid_loss = []\n",
    "    \n",
    "    early_stopping = EarlyStopping(patience = patience, verbose = True)\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        for batch, (X, y) in enumerate(train_dl):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # Compute prediction and loss\n",
    "            pred = model(X)\n",
    "            loss = loss_func(pred, y)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "    model.eval()\n",
    "    for data, target in val_dl:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        \n",
    "        val_losses.append(loss.item())\n",
    "        \n",
    "        train_loss = np.mean(train_losses)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        avg_train_loss.append(train_loss)\n",
    "        avg_val_loss.append(val_loss)\n",
    "\n",
    "        epoch_len = len(str(n_epochs)) # ??\n",
    "\n",
    "        print_msg = (f'[{epoch:>{epoch_len}}/{n_epochs:>{epoch_len}}] ' + \n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'val_loss: {val_loss:.5f}')\n",
    "\n",
    "        print(print_msg)\n",
    "\n",
    "        train_losses = []\n",
    "        valid_losses = []\n",
    "\n",
    "        early_stopping(val_loss, model)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "    return  model, avg_train_losses, avg_valid_losses\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "# loss_func = nn.CrossEntropyLoss().to(device)\n",
    "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "patience = 20\n",
    "n_epochs = 100\n",
    "\n",
    "model, train_loss, val_loss = fit(model, patience, loss_func, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법 3\n",
    "- `(2017) DenseNet.ipynb`의 학습 코드 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T07:19:00.592692Z",
     "start_time": "2022-04-11T07:19:00.578575Z"
    }
   },
   "outputs": [],
   "source": [
    "# define loss function, optimizer, lr_scheduler\n",
    "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
    "opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "lr_scheduler = ReduceLROnPlateau(opt, mode='min', factor=0.1, patience=8)\n",
    "\n",
    "\n",
    "# get current lr\n",
    "def get_lr(opt):\n",
    "    for param_group in opt.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "\n",
    "# calculate the metric per mini-batch\n",
    "def metric_batch(output, target):\n",
    "    pred = output.argmax(1, keepdim=True)\n",
    "    corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return corrects\n",
    "\n",
    "\n",
    "# calculate the loss per mini-batch\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "    loss_b = loss_func(output, target)\n",
    "    metric_b = metric_batch(output, target)\n",
    "\n",
    "    if opt is not None:\n",
    "        opt.zero_grad()\n",
    "        loss_b.backward()\n",
    "        opt.step()\n",
    "    \n",
    "    return loss_b.item(), metric_b\n",
    "\n",
    "\n",
    "# calculate the loss per epochs\n",
    "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False, opt=None):\n",
    "    running_loss = 0.0\n",
    "    running_metric = 0.0\n",
    "    len_data = len(dataset_dl.dataset)\n",
    "\n",
    "    for xb, yb in dataset_dl:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        output = model(xb)\n",
    "\n",
    "        loss_b, metric_b = loss_batch(loss_func, output, yb, opt)\n",
    "\n",
    "        running_loss += loss_b\n",
    "        \n",
    "        if metric_b is not None:\n",
    "            running_metric += metric_b\n",
    "\n",
    "        if sanity_check is True:\n",
    "            break\n",
    "\n",
    "    loss = running_loss / len_data\n",
    "    metric = running_metric / len_data\n",
    "    return loss, metric\n",
    "\n",
    "\n",
    "# function to start training\n",
    "def train_val(model, params):\n",
    "    num_epochs=params['num_epochs']\n",
    "    loss_func=params['loss_func']\n",
    "    opt=params['optimizer']\n",
    "    train_dl=params['train_dl']\n",
    "    val_dl=params['val_dl']\n",
    "    sanity_check=params['sanity_check']\n",
    "    lr_scheduler=params['lr_scheduler']\n",
    "    path2weights=params['path2weights']\n",
    "\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "    metric_history = {'train': [], 'val': []}\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_lr = get_lr(opt)\n",
    "        print('Epoch {}/{}, current lr= {}'.format(epoch, num_epochs-1, current_lr))\n",
    "\n",
    "        model.train()\n",
    "        train_loss, train_metric = loss_epoch(model, loss_func, train_dl, sanity_check, opt)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        metric_history['train'].append(train_metric)\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_metric = loss_epoch(model, loss_func, val_dl, sanity_check)\n",
    "        loss_history['val'].append(val_loss)\n",
    "        metric_history['val'].append(val_metric)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            torch.save(model.state_dict(), path2weights)\n",
    "            print('Copied best model weights!')\n",
    "\n",
    "        lr_scheduler.step(val_loss)\n",
    "        if current_lr != get_lr(opt):\n",
    "            print('Loading best model weights!')\n",
    "            model.load_state_dict(best_model_wts)\n",
    "\n",
    "        print('train loss: %.6f, val loss: %.6f, accuracy: %.2f, time: %.4f min' %(train_loss, val_loss, 100*val_metric, (time.time()-start_time)/60))\n",
    "        print('-'*10)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T07:19:08.613910Z",
     "start_time": "2022-04-11T07:19:08.610331Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the training parameters\n",
    "params_train = {\n",
    "    'num_epochs':3,\n",
    "    'optimizer':opt,\n",
    "    'loss_func':loss_func,\n",
    "    'train_dl':train_dl,\n",
    "    'val_dl':val_dl,\n",
    "    'sanity_check':False,\n",
    "    'lr_scheduler':lr_scheduler,\n",
    "    'path2weights':'./models/weights.pt',\n",
    "}\n",
    "\n",
    "# check the directory to save weights.pt\n",
    "def createFolder(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSerror:\n",
    "        print('Error')\n",
    "createFolder('./models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-11T07:19:11.908757Z",
     "start_time": "2022-04-11T07:19:08.735055Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/2, current lr= 0.01\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, loss_hist, metric_hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mtrain_val\u001b[0;34m(model, params)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, current lr= \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, num_epochs\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, current_lr))\n\u001b[1;32m     83\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 84\u001b[0m train_loss, train_metric \u001b[38;5;241m=\u001b[39m \u001b[43mloss_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msanity_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m loss_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[1;32m     86\u001b[0m metric_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(train_metric)\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mloss_epoch\u001b[0;34m(model, loss_func, dataset_dl, sanity_check, opt)\u001b[0m\n\u001b[1;32m     43\u001b[0m yb \u001b[38;5;241m=\u001b[39m yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m output \u001b[38;5;241m=\u001b[39m model(xb)\n\u001b[0;32m---> 46\u001b[0m loss_b, metric_b \u001b[38;5;241m=\u001b[39m \u001b[43mloss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_b\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metric_b \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mloss_batch\u001b[0;34m(loss_func, output, target, opt)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_batch\u001b[39m(loss_func, output, target, opt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 24\u001b[0m     loss_b \u001b[38;5;241m=\u001b[39m \u001b[43mloss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     metric_b \u001b[38;5;241m=\u001b[39m metric_batch(output, target)\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m opt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/loss.py:1163\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/functional.py:2996\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2995\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not tuple"
     ]
    }
   ],
   "source": [
    "model, loss_hist, metric_hist = train_val(model, params_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 방법 4\n",
    "- [Cifar100 with CNN from Scratch and GoogLeNet](https://www.kaggle.com/code/masrur007/cifar100-with-cnn-from-scratch-and-googlenet#Step-4-Test) + 정확도 추가 버전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-04-13T02:28:59.431Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# filter(함수, 리스트) : 리스트에 들어있는 원소들을 함수에 적용시켜서 결과가 참인 값들로 새로운 리스트 생성 \n",
    "# model_transfer의 parameter 중에서 grad의 변화도를 갖는 parameter만 가진 리스트 생성\n",
    "model_grad_parameters = filter(lambda p: p.requires_grad, model2.parameters())\n",
    "optimizer = torch.optim.Adam(model_grad_parameters, lr=0.001)\n",
    "\n",
    "# 220413 wed 정확도가 net보다 개선되지 않는 문제..를 해결하기 위한 일환으로 시도\n",
    "# 결과적으로 큰 영향은 없었고, 정확도가 net보다 낮은 게 정상임\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "lr_scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "n_epochs = 10\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # --------------- #\n",
    "    # train the model #\n",
    "    # --------------- #\n",
    "    model2.train()\n",
    "    for data, target in train_dl: # target.shape = [32]\n",
    "\n",
    "#         if torch.cuda.is_available():\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad() # train only\n",
    "        output = model2(data) # output.shape = [32, 10]\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward() # train only\n",
    "        optimizer.step() # train only\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # ------------------ #\n",
    "    # validate the model #\n",
    "    # ------------------ #\n",
    "    model2.eval()\n",
    "    sum_correct = 0\n",
    "    for data, target in val_dl:\n",
    "#         if torch.cuda.is_available():\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        output = model2(data)\n",
    "        \n",
    "        # accuracy\n",
    "        pred = output.argmax(dim=1, keepdim=True) \n",
    "        corrects = pred.eq(target.view_as(pred)).sum().item()\n",
    "        sum_correct += corrects\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    train_loss = train_loss / len(train_dl.sampler)\n",
    "    valid_loss = valid_loss / len(val_dl.sampler)\n",
    "    accuracy = sum_correct / len(val_dl.sampler) * 100\n",
    "    \n",
    "    print(f'Epoch {epoch}/{n_epochs} \\t train_loss: {train_loss:.3f} \\t val_loss: {valid_loss:.3f} \\t accuracy: {accuracy:.3f}')\n",
    "    print('------------------')\n",
    "    \n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('val_loss decreased [ {:.3f} --> {:.3f} | diff: {:.3f} ] Saving model...'.format(valid_loss_min, valid_loss, valid_loss_min-valid_loss))\n",
    "        torch.save(model_transfer.state_dict(), 'model_transfer_cifar.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GoogLeNet(2014).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
