{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f9ab67d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T07:37:47.933860Z",
     "start_time": "2022-04-14T07:37:47.926399Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import cv2\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# 결과 확인을 용이하게 하기 위한 코드\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "903080a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T07:37:49.284202Z",
     "start_time": "2022-04-14T07:37:47.935768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "42500\n",
      "7500\n"
     ]
    }
   ],
   "source": [
    "# Define transform options\n",
    "train_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.RandomCrop(227),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "                                     ])\n",
    "                                      \n",
    "test_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "                                     ])\n",
    "\n",
    "# Data load\n",
    "train_data = datasets.CIFAR10('./data', download = True, train = True, transform = train_transform) # len : 50000\n",
    "test_data = datasets.CIFAR10('./data', download = True, train = False, transform = test_transform) # len : 10000\n",
    "\n",
    "# Train, valid split\n",
    "valid_size = 0.15\n",
    "indices = list(range(len(train_data)))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * len(train_data)))\n",
    "\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "print(len(train_idx))\n",
    "print(len(valid_idx))\n",
    "\n",
    "# Define samplers\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# Dataloaders (Combine dataset and sampler)\n",
    "batch_size = 256\n",
    "train_dl = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = train_sampler)\n",
    "val_dl = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = train_sampler)\n",
    "test_dl = torch.utils.data.DataLoader(test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85bfea09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T07:37:49.287503Z",
     "start_time": "2022-04-14T07:37:49.285369Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85692053",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T07:37:49.294217Z",
     "start_time": "2022-04-14T07:37:49.289511Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install tensorboardcolab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "564f8a53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T07:47:37.544401Z",
     "start_time": "2022-04-14T07:47:32.198548Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback, TensorBoard\n",
    "from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.nn import local_response_normalization  # lrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af6ff7b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T07:47:37.818490Z",
     "start_time": "2022-04-14T07:47:37.546750Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-14 16:47:37.563079: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " INPUT (InputLayer)          [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " CONV_1 (Conv2D)             (None, 54, 54, 96)        34944     \n",
      "                                                                 \n",
      " POOL_1 (MaxPooling2D)       (None, 26, 26, 96)        0         \n",
      "                                                                 \n",
      " LRN_1 (BatchNormalization)  (None, 26, 26, 96)        384       \n",
      "                                                                 \n",
      " CONV_2 (Conv2D)             (None, 26, 26, 256)       614656    \n",
      "                                                                 \n",
      " POOL_2 (MaxPooling2D)       (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " LRN_2 (BatchNormalization)  (None, 12, 12, 256)       1024      \n",
      "                                                                 \n",
      " CONV_3 (Conv2D)             (None, 12, 12, 384)       885120    \n",
      "                                                                 \n",
      " CONV_4 (Conv2D)             (None, 12, 12, 384)       1327488   \n",
      "                                                                 \n",
      " CONV_5 (Conv2D)             (None, 12, 12, 256)       884992    \n",
      "                                                                 \n",
      " POOL_3 (MaxPooling2D)       (None, 5, 5, 256)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " FC_1 (Dense)                (None, 4096)              26218496  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " FC_2 (Dense)                (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " OUTPUT (Dense)              (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,845,416\n",
      "Trainable params: 50,844,712\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# modeling(functional API)\n",
    "input_shape = (224, 224, 3)  # 논문에서 제시된 shape\n",
    "x = Input(shape = input_shape, name='INPUT')\n",
    "\n",
    "# CONV\n",
    "conv1 = Conv2D(filters=96, kernel_size=11, activation='relu', strides=4, name='CONV_1')(x)\n",
    "pool1 = MaxPooling2D((3,3), strides=2, name='POOL_1')(conv1)  # overlapped pooling\n",
    "# lrn1 = local_response_normalization(conv1,depth_radius=5, bias=2, alpha=0.0001, beta=0.75) \n",
    "lrn1 = BatchNormalization(name='LRN_1')(pool1)\n",
    "\n",
    "conv2 = Conv2D(filters=256, kernel_size=5, activation='relu', strides=1, padding='same', name='CONV_2')(lrn1)\n",
    "pool2 = MaxPooling2D((3,3), strides=2, name='POOL_2')(conv2)\n",
    "# lrn2 = local_response_normalization(conv2,depth_radius=5, bias=2,  alpha=0.0001, beta=0.75)\n",
    "lrn2 = BatchNormalization(name='LRN_2')(pool2)\n",
    "\n",
    "conv3 = Conv2D(filters=384, kernel_size=3, activation='relu', strides=1, padding='same', name='CONV_3')(lrn2)\n",
    "conv4 = Conv2D(filters=384, kernel_size=3, activation='relu', strides=1, padding='same', name='CONV_4')(conv3)\n",
    "conv5 = Conv2D(filters=256, kernel_size=3, activation='relu', strides=1, padding='same', name='CONV_5')(conv4)\n",
    "pool3 = MaxPooling2D((3,3), strides=2, name='POOL_3')(conv5)\n",
    "\n",
    "# FC\n",
    "f = Flatten()(pool3)\n",
    "f = Dense(4096, activation='relu', name='FC_1')(f)\n",
    "f = Dropout(0.5)(f)  # 논문 parameter 0.5 이용\n",
    "f = Dense(4096, activation='relu', name='FC_2')(f)\n",
    "f = Dropout(0.5)(f)\n",
    "out = Dense(1000, activation='softmax', name='OUTPUT')(f)\n",
    "\n",
    "model = Model(inputs=x, outputs=out)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a406cdb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T08:23:23.560925Z",
     "start_time": "2022-04-14T07:47:37.820255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2143s 13us/step\n",
      "170508288/170498071 [==============================] - 2143s 13us/step\n"
     ]
    }
   ],
   "source": [
    "# cifar10 data load\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d350ccc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T09:00:03.165674Z",
     "start_time": "2022-04-14T09:00:03.161171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e2bc6a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T09:01:06.954262Z",
     "start_time": "2022-04-14T09:01:06.396377Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3072 into shape (1,2,0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/gz/l3tl4gf96231sq9ztwck08x00000gp/T/ipykernel_15746/737034004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata_upscaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m227\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m227\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlarge_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m227\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m227\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINTER_CUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     large_img = cv2.resize(x_train, dsize=(227, 227), interpolation=cv2.INTER_AREA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 3072 into shape (1,2,0)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## 227, 227로 Resize 시도 -> RAM 초과 \n",
    "from keras.datasets import cifar10\n",
    "import cv2\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "data_upscaled = np.zeros((50000, 3, 227, 227))\n",
    "for i, img in enumerate(x_train):\n",
    "    im = img.reshape((1, 2, 0))\n",
    "    large_img = cv2.resize(im, dsize=(227, 227), interpolation=cv2.INTER_CUBIC)\n",
    "#     large_img = cv2.resize(x_train, dsize=(227, 227), interpolation=cv2.INTER_AREA)\n",
    "    data_upscaled[i] = large_img.reshape((2, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf72d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46a5a81e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T08:31:11.776273Z",
     "start_time": "2022-04-14T08:31:11.658802Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)  # category화 \n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Conv2D(96, (11,11), strides=(4,4), activation='relu', padding='same', input_shape=(img_height, img_width, channel,)))\n",
    "# for original Alexnet\n",
    "model.add(Conv2D(48, (3,3), strides=(2,2), activation='relu', padding='same', input_shape=(img_height, img_width, channel,)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
    "# Local Response normalization for Original Alexnet\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(96, (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
    "# Local Response normalization for Original Alexnet\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(192, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(192, (3,3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3,3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), strides=(2,2)))\n",
    "# Local Response normalization for Original Alexnet\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='tanh'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b611a1fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T08:37:02.872417Z",
     "start_time": "2022-04-14T08:31:11.777820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "391/391 [==============================] - 37s 93ms/step - loss: 1.5531 - accuracy: 0.4523 - val_loss: 1.5336 - val_accuracy: 0.4825\n",
      "Epoch 2/10\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 1.0999 - accuracy: 0.6188 - val_loss: 1.0576 - val_accuracy: 0.6314\n",
      "Epoch 3/10\n",
      "391/391 [==============================] - 35s 90ms/step - loss: 0.9168 - accuracy: 0.6855 - val_loss: 1.0726 - val_accuracy: 0.6298\n",
      "Epoch 4/10\n",
      "391/391 [==============================] - 34s 87ms/step - loss: 0.7964 - accuracy: 0.7284 - val_loss: 0.9223 - val_accuracy: 0.6878\n",
      "Epoch 5/10\n",
      "391/391 [==============================] - 33s 84ms/step - loss: 0.6984 - accuracy: 0.7649 - val_loss: 1.0117 - val_accuracy: 0.6637\n",
      "Epoch 6/10\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.6204 - accuracy: 0.7933 - val_loss: 0.9693 - val_accuracy: 0.6882\n",
      "Epoch 7/10\n",
      "391/391 [==============================] - 35s 89ms/step - loss: 0.5401 - accuracy: 0.8203 - val_loss: 1.0505 - val_accuracy: 0.6866\n",
      "Epoch 8/10\n",
      "391/391 [==============================] - 34s 88ms/step - loss: 0.4597 - accuracy: 0.8462 - val_loss: 1.2624 - val_accuracy: 0.6665\n",
      "Epoch 9/10\n",
      "391/391 [==============================] - 38s 96ms/step - loss: 0.4046 - accuracy: 0.8635 - val_loss: 0.9936 - val_accuracy: 0.7075\n",
      "Epoch 10/10\n",
      "391/391 [==============================] - 37s 95ms/step - loss: 0.3457 - accuracy: 0.8871 - val_loss: 1.0593 - val_accuracy: 0.7068\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10, \n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
